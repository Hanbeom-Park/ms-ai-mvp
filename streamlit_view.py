from typing import Any, Dict
import pandas as pd
import re
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
from agent import create_agent_graph

def preprocess_dynamic(df: pd.DataFrame) -> list:
    # 1. ì»¬ëŸ¼ëª… í‘œì¤€í™”
    new_columns = []
    for col in df.columns:
        col_clean = col.strip().lower()                  # ê³µë°± ì œê±° + ì†Œë¬¸ì
        col_clean = re.sub(r"\s+", "_", col_clean)        # ì—¬ëŸ¬ ê³µë°± â†’ "_"
        col_clean = col_clean.replace("%", "pct")         # % â†’ pct
        col_clean = col_clean.replace("(", "").replace(")", "")
        new_columns.append(col_clean)
    df.columns = new_columns

    # 2. ê°’ ì „ì²˜ë¦¬ (ë‹¨ìœ„ ë³€í™˜ ì—†ìŒ)
    for col in df.columns:
        if df[col].dtype == object:
            df[col] = df[col].astype(str).str.strip()  # ë¬¸ìì—´ ê³µë°± ì œê±°
        df[col] = df[col].fillna("")  # NaN â†’ ë¹ˆ ë¬¸ìì—´

    # 3. ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates()

    # 4. JSON ë³€í™˜
    json_data = df.to_dict(orient="records")
    return json_data


# Custom callback handler for Streamlit streaming
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, placeholder):
        self.placeholder = placeholder
        self.full_response = ""
        self.cursor = "â–Œ"
        self.tool_output = ""
        self.is_tool_running = False
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        self.full_response += token
        self.placeholder.markdown(self.full_response + self.cursor)
        
    def on_llm_end(self, response, **kwargs) -> None:
        """Run when LLM ends running."""
        self.placeholder.markdown(self.full_response)
        
    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs) -> None:
        """Run when tool starts running."""
        self.is_tool_running = True
        tool_name = serialized.get("name", "tool")
        self.full_response += f"\n\n*ë„êµ¬ ì‹¤í–‰ ì¤‘: {tool_name}...*\n"
        self.placeholder.markdown(self.full_response + self.cursor)

    def on_tool_end(self, output: str, **kwargs) -> None:
        """Run when tool ends running."""
        self.full_response = ""
        self.is_tool_running = False
        self.placeholder.markdown(self.full_response + self.cursor)
        
    def on_agent_action(self, action, **kwargs) -> Any:
        """Run on agent action."""
        pass
        
    def on_agent_finish(self, finish, **kwargs) -> None:
        """Run on agent end."""
        pass

st.set_page_config(page_title="Azure Migration Agent", page_icon="ğŸ¤–")
st.title("Azure Migration Agent")

# ì„œë¹„ìŠ¤ ì •ë³´ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "service_type" not in st.session_state:
    st.session_state.service_type = None
if "network_type" not in st.session_state:
    st.session_state.network_type = None
if "service_abbr" not in st.session_state:
    st.session_state.service_abbr = ""
if "nas_usage" not in st.session_state:
    st.session_state.nas_usage = False

# ì¶”ê°€ ì •ë³´ ì…ë ¥ ì„¹ì…˜
st.sidebar.title("ì„œë¹„ìŠ¤ ì •ë³´ ì…ë ¥")
service_type = st.sidebar.selectbox("ì„œë¹„ìŠ¤ ìœ í˜•", [None, "VM ê¸°ë°˜ ì„œë¹„ìŠ¤", "AKS ì„œë¹„ìŠ¤"], index=0 if st.session_state.service_type is None else 
                                   (1 if st.session_state.service_type == "VM ê¸°ë°˜ ì„œë¹„ìŠ¤" else 2))
network_type = st.sidebar.selectbox("ë„¤íŠ¸ì›Œí¬ ì—°ë™ ìœ í˜•", [None, "ê³µì¸ë§ ì—°ë™", "ë‚´ë¶€ë§ ì—°ë™"], index=0 if st.session_state.network_type is None else 
                                   (1 if st.session_state.network_type == "ê³µì¸ë§ ì—°ë™" else 2))
service_abbr = st.sidebar.text_input("ì„œë¹„ìŠ¤ ì•½ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", value=st.session_state.service_abbr)
nas_usage = st.sidebar.checkbox("NAS ì‚¬ìš© ì—¬ë¶€", value=st.session_state.nas_usage)

# ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
st.session_state.service_type = service_type
st.session_state.network_type = network_type
st.session_state.service_abbr = service_abbr
st.session_state.nas_usage = nas_usage

# íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
uploaded_file = st.sidebar.file_uploader("Excel ë˜ëŠ” CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx", "csv"])

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processed_data" not in st.session_state:
    st.session_state.processed_data = None
if "filtered_df" not in st.session_state:
    st.session_state.filtered_df = None
if "selected_columns" not in st.session_state:
    st.session_state.selected_columns = None
if "json_data" not in st.session_state:
    st.session_state.json_data = None

# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
def process_file(file):
    try:
        if file.name.endswith('.csv'):
            df = pd.read_csv(file)
        else:  # Excel íŒŒì¼
            df = pd.read_excel(file)
        
        # preprocess_dynamic ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•íƒœë¡œ ì „ì²˜ë¦¬
        json_data = preprocess_dynamic(df)
        
        return df, json_data
    except Exception as e:
        st.sidebar.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None


# íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
if uploaded_file is not None:
    st.sidebar.info(f"íŒŒì¼ '{uploaded_file.name}'ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # íŒŒì¼ ì²˜ë¦¬
    processed_df, json_data = process_file(uploaded_file)
    if processed_df is not None:
        st.session_state.processed_data = processed_df
        st.session_state.json_data = json_data
        
        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.sidebar.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.sidebar.dataframe(processed_df.head())
        
        # ê¸°ë³¸ ì»¬ëŸ¼ ì •ë³´ ì„¤ì •
        all_columns = processed_df.columns.tolist()
        st.session_state.selected_columns = all_columns
        st.session_state.filtered_df = processed_df
        
        
        # ì—ì´ì „íŠ¸ì—ê²Œ íŒŒì¼ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë„ë¡ ì•ˆë‚´
        st.sidebar.info("ì±„íŒ…ì°½ì—ì„œ ì—…ë¡œë“œí•œ íŒŒì¼ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # íŒŒì¼ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, í”„ë¡¬í”„íŠ¸ì— íŒŒì¼ ì •ë³´ì™€ ë°ì´í„° ì¶”ê°€
    if st.session_state.processed_data is not None:
        # í•„í„°ë§ëœ ë°ì´í„° ì‚¬ìš©
        if st.session_state.filtered_df is not None:
            df = st.session_state.filtered_df
            original_df = st.session_state.processed_data
            is_filtered = df.shape[0] != original_df.shape[0]
        else:
            df = st.session_state.processed_data
            is_filtered = False
        
        # ì„ íƒëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        if st.session_state.selected_columns:
            selected_cols = st.session_state.selected_columns
            df_selected = df[selected_cols]
        else:
            selected_cols = df.columns.tolist()
            df_selected = df
        
        # ê¸°ë³¸ íŒŒì¼ ì •ë³´ ì¶”ê°€
        if is_filtered:
            file_info = f"ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´: ì „ì²´ {original_df.shape[0]}í–‰ ì¤‘ {df.shape[0]}í–‰ í•„í„°ë§ë¨, {len(selected_cols)}ì—´, ì„ íƒëœ ì»¬ëŸ¼: {', '.join(selected_cols)}"
        else:
            file_info = f"ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´: {df.shape[0]}í–‰ x {len(selected_cols)}ì—´, ì„ íƒëœ ì»¬ëŸ¼: {', '.join(selected_cols)}"
        
        # ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ ì¶”ê°€ (ì²˜ìŒ 10ê°œ í–‰ ë˜ëŠ” ì „ì²´ í–‰ì´ 10ê°œ ë¯¸ë§Œì¸ ê²½ìš° ì „ì²´)
        sample_rows = min(10, df.shape[0])
        data_sample = df_selected.head(sample_rows).to_string(index=False)
        
        # ë°ì´í„°ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„ ì •ë³´ ì¶”ê°€
        try:
            numeric_stats = df_selected.describe().to_string()
            stats_info = f"\n\në°ì´í„° í†µê³„ ì •ë³´:\n{numeric_stats}"
        except:
            stats_info = ""
        
        # ë°ì´í„° ìœ í˜• ì •ë³´ ì¶”ê°€
        dtypes_info = "\n\në°ì´í„° ìœ í˜•:\n" + "\n".join([f"{col}: {df_selected[col].dtype}" for col in selected_cols])
        
        # ê²°ì¸¡ì¹˜ ì •ë³´ ì¶”ê°€
        null_counts = df_selected.isnull().sum()
        null_info = "\n\nê²°ì¸¡ì¹˜ ì •ë³´:\n" + "\n".join([f"{col}: {null_counts[col]}ê°œ" for col in selected_cols])
        
        
        # ì„œë¹„ìŠ¤ ì •ë³´ ì¶”ê°€
        service_info = "\n\nì„œë¹„ìŠ¤ ì •ë³´:"
        if service_type is not None:
            service_info += f"\n- ì„œë¹„ìŠ¤ ìœ í˜•: {service_type}"
        if network_type is not None:
            service_info += f"\n- ë„¤íŠ¸ì›Œí¬ ì—°ë™ ìœ í˜•: {network_type}"
        if service_abbr:
            service_info += f"\n- ì„œë¹„ìŠ¤ ì•½ì–´: {service_abbr}"
        service_info += f"\n- NAS ì‚¬ìš© ì—¬ë¶€: {'ì˜ˆ' if nas_usage else 'ì•„ë‹ˆì˜¤'}"
        
        # JSON ë°ì´í„° ì¶”ê°€ (ìµœëŒ€ 10ê°œ í•­ëª©)
        json_sample = str(st.session_state.json_data[:10]) if st.session_state.json_data else ""
        
        enhanced_prompt = f"{prompt}\n\n[íŒŒì¼ ì»¨í…ìŠ¤íŠ¸: {file_info}\n\në°ì´í„° ìƒ˜í”Œ:\n{data_sample}{stats_info}{dtypes_info}{null_info}\n\nJSON ë°ì´í„°:\n{json_sample}{service_info}]"
    else:
        # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì—ë„ ì„œë¹„ìŠ¤ ì •ë³´ ì¶”ê°€
        service_info = "\n\nì„œë¹„ìŠ¤ ì •ë³´:"
        if service_type is not None:
            service_info += f"\n- ì„œë¹„ìŠ¤ ìœ í˜•: {service_type}"
        if network_type is not None:
            service_info += f"\n- ë„¤íŠ¸ì›Œí¬ ì—°ë™ ìœ í˜•: {network_type}"
        if service_abbr:
            service_info += f"\n- ì„œë¹„ìŠ¤ ì•½ì–´: {service_abbr}"
        service_info += f"\n- NAS ì‚¬ìš© ì—¬ë¶€: {'ì˜ˆ' if nas_usage else 'ì•„ë‹ˆì˜¤'}"
        enhanced_prompt = f"{prompt}{service_info}"
    
    
    with st.chat_message("assistant"):
        # Create a placeholder for streaming output
        message_placeholder = st.empty()
        
        # Create a callback handler for streaming
        callback_handler = StreamlitCallbackHandler(message_placeholder)
        
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            # Use streaming callback
            response = create_agent_graph().invoke(
                {"input": enhanced_prompt},
                {"callbacks": [callback_handler]}
            )
            
            # Get the final answer - extract only the "answer" field from AgentState
            answer = response.get("answer", "")
            
            # Save to session state
            st.session_state.messages.append({"role": "assistant", "content": answer})